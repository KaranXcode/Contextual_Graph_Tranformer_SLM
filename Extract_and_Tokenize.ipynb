{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6446b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import docx2txt\n",
    "import re\n",
    "import csv\n",
    "from io import StringIO\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check PyMuPDF version\n",
    "if not hasattr(fitz.Page, \"find_tables\"):\n",
    "    logger.error(\"PyMuPDF version does not support table extraction. Please upgrade to version 1.23.0 or higher.\")\n",
    "    raise ImportError(\"PyMuPDF version too old. Run 'pip install --upgrade pymupdf'.\")\n",
    "\n",
    "# Paths\n",
    "root_folder = Path(r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\abb_products_pdf_files\")\n",
    "root2_folder = Path(r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\")\n",
    "output_folder = root2_folder / \"processed_final_grok\"\n",
    "individual_docs_folder = output_folder / \"individual_docs\"\n",
    "images_folder = output_folder / \"extracted_images\"\n",
    "individual_docs_folder.mkdir(parents=True, exist_ok=True)\n",
    "images_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Noise patterns\n",
    "noise_patterns = [\n",
    "    r\"^Contents\\b.*\", r\"^Page\\s+\\d+\", r\"^ABB\\s*$\",\n",
    "    r\"^Product version.*\", r\"^Issued.*\", r\"^Revision.*\",\n",
    "    r\"^Figure\\s+\\d+\\..*\", r\"^GUID-[\\w\\-]+\", r\"^¬© Copyright.*\",\n",
    "    r\"^All rights reserved.*\", r\"^Trademarks.*\", r\"^Disclaimer.*\",\n",
    "    r\"^http[s]?://.*\", r\"^\\s*-+\\s*$\", r\"^\\s*$\"\n",
    "]\n",
    "noise_regex = re.compile(\"|\".join(noise_patterns), re.IGNORECASE)\n",
    "\n",
    "# Utility functions\n",
    "def is_near_image(block_rect, image_rects, margin=30):\n",
    "    for img in image_rects:\n",
    "        if (block_rect.intersects(img) or\n",
    "            abs(block_rect.y0 - img.y1) < margin or\n",
    "            abs(block_rect.y1 - img.y0) < margin):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_meaningful(line):\n",
    "    words = line.split()\n",
    "    if len(words) < 4:\n",
    "        return False\n",
    "    alpha_ratio = sum(c.isalpha() for c in line) / len(line)\n",
    "    return alpha_ratio > 0.5\n",
    "\n",
    "def extract_text_from_table_region(page, table_bbox):\n",
    "    \"\"\"Extract text from table region using text blocks method as fallback.\"\"\"\n",
    "    try:\n",
    "        # Get text blocks within the table bounding box\n",
    "        table_rect = fitz.Rect(table_bbox)\n",
    "        blocks = page.get_text(\"dict\", clip=table_rect)\n",
    "        \n",
    "        # Extract text from blocks and organize by position\n",
    "        text_items = []\n",
    "        for block in blocks.get(\"blocks\", []):\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        if text:\n",
    "                            bbox = span[\"bbox\"]\n",
    "                            text_items.append({\n",
    "                                \"text\": text,\n",
    "                                \"x0\": bbox[0],\n",
    "                                \"y0\": bbox[1],\n",
    "                                \"x1\": bbox[2],\n",
    "                                \"y1\": bbox[3]\n",
    "                            })\n",
    "        \n",
    "        # Sort by vertical position first, then horizontal\n",
    "        text_items.sort(key=lambda x: (x[\"y0\"], x[\"x0\"]))\n",
    "        \n",
    "        # Group items into rows based on vertical position\n",
    "        rows = []\n",
    "        current_row = []\n",
    "        current_y = None\n",
    "        tolerance = 5  # pixels tolerance for same row\n",
    "        \n",
    "        for item in text_items:\n",
    "            if current_y is None or abs(item[\"y0\"] - current_y) <= tolerance:\n",
    "                current_row.append(item)\n",
    "                current_y = item[\"y0\"] if current_y is None else current_y\n",
    "            else:\n",
    "                if current_row:\n",
    "                    # Sort current row by horizontal position\n",
    "                    current_row.sort(key=lambda x: x[\"x0\"])\n",
    "                    rows.append([item[\"text\"] for item in current_row])\n",
    "                current_row = [item]\n",
    "                current_y = item[\"y0\"]\n",
    "        \n",
    "        # Add the last row\n",
    "        if current_row:\n",
    "            current_row.sort(key=lambda x: x[\"x0\"])\n",
    "            rows.append([item[\"text\"] for item in current_row])\n",
    "        \n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting text from table region: {e}\")\n",
    "        return []\n",
    "\n",
    "def format_table_to_text(page, table):\n",
    "    \"\"\"Convert a table to a CSV-like text format with proper text extraction.\"\"\"\n",
    "    try:\n",
    "        if not table:\n",
    "            logger.warning(\"Empty table encountered.\")\n",
    "            return \"\"\n",
    "        \n",
    "        output = StringIO()\n",
    "        writer = csv.writer(output, lineterminator='\\n')\n",
    "        \n",
    "        # First, try to extract using PyMuPDF's table extraction\n",
    "        try:\n",
    "            table_data = table.extract()\n",
    "            if table_data and len(table_data) > 0:\n",
    "                # Check if we got actual text content\n",
    "                sample_cell = str(table_data[0][0]) if table_data[0] else \"\"\n",
    "                if not (sample_cell.startswith(\"(\") and \",\" in sample_cell and sample_cell.endswith(\")\")):\n",
    "                    # We got actual text, not coordinates\n",
    "                    for row in table_data:\n",
    "                        cleaned_row = []\n",
    "                        for cell in row:\n",
    "                            cell_text = str(cell).strip() if cell is not None else \"\"\n",
    "                            cleaned_row.append(cell_text)\n",
    "                        if any(cleaned_row):  # Only add non-empty rows\n",
    "                            writer.writerow(cleaned_row)\n",
    "                    \n",
    "                    table_text = output.getvalue()\n",
    "                    output.close()\n",
    "                    return table_text if table_text.strip() else \"\"\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"PyMuPDF table extraction failed: {e}\")\n",
    "        \n",
    "        # Fallback: Extract text from table region using coordinate-based method\n",
    "        logger.info(\"Using fallback text extraction for table\")\n",
    "        table_bbox = table.bbox\n",
    "        rows = extract_text_from_table_region(page, table_bbox)\n",
    "        \n",
    "        if rows:\n",
    "            for row in rows:\n",
    "                if any(cell.strip() for cell in row):  # Only add rows with content\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            table_text = output.getvalue()\n",
    "            output.close()\n",
    "            return table_text if table_text.strip() else \"\"\n",
    "        \n",
    "        output.close()\n",
    "        return \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error formatting table: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_tables_with_textboxes(page):\n",
    "    \"\"\"Alternative table extraction using text positioning analysis.\"\"\"\n",
    "    try:\n",
    "        # Get all text with detailed positioning\n",
    "        text_dict = page.get_text(\"dict\")\n",
    "        \n",
    "        # Collect all text elements with their positions\n",
    "        text_elements = []\n",
    "        for block in text_dict.get(\"blocks\", []):\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        if text and len(text) > 1:  # Filter out single characters\n",
    "                            bbox = span[\"bbox\"]\n",
    "                            text_elements.append({\n",
    "                                \"text\": text,\n",
    "                                \"x0\": bbox[0],\n",
    "                                \"y0\": bbox[1],\n",
    "                                \"x1\": bbox[2],\n",
    "                                \"y1\": bbox[3],\n",
    "                                \"size\": span[\"size\"]\n",
    "                            })\n",
    "        \n",
    "        # Group elements that might form tables (similar y-coordinates, aligned x-coordinates)\n",
    "        potential_tables = []\n",
    "        processed_elements = set()\n",
    "        \n",
    "        for i, elem in enumerate(text_elements):\n",
    "            if i in processed_elements:\n",
    "                continue\n",
    "                \n",
    "            # Look for elements at similar y-level (potential row)\n",
    "            row_elements = [elem]\n",
    "            processed_elements.add(i)\n",
    "            \n",
    "            for j, other_elem in enumerate(text_elements[i+1:], i+1):\n",
    "                if j in processed_elements:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if elements are on the same row (similar y-coordinates)\n",
    "                if abs(elem[\"y0\"] - other_elem[\"y0\"]) <= 3:\n",
    "                    row_elements.append(other_elem)\n",
    "                    processed_elements.add(j)\n",
    "            \n",
    "            # If we found multiple elements in a row, it might be a table row\n",
    "            if len(row_elements) >= 2:\n",
    "                row_elements.sort(key=lambda x: x[\"x0\"])  # Sort by x-coordinate\n",
    "                potential_tables.append([elem[\"text\"] for elem in row_elements])\n",
    "        \n",
    "        return potential_tables\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Alternative table extraction failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main PDF text and table extractor\n",
    "def extract_clean_text_from_pdf(file_path):\n",
    "    text_blocks = []\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        first_heading = None\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Capture the title on the first page\n",
    "            if page_num == 0:\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                titles = [\n",
    "                    block[\"lines\"][0][\"spans\"][0][\"text\"].strip()\n",
    "                    for block in blocks\n",
    "                    if block.get(\"lines\") and block[\"lines\"][0][\"spans\"] and block[\"lines\"][0][\"spans\"][0][\"size\"] > 15\n",
    "                ]\n",
    "                if titles:\n",
    "                    first_heading = \" \".join(titles).strip()\n",
    "            \n",
    "            # Extract images to avoid text near them\n",
    "            image_rects = [fitz.Rect(img[1:5]) for img in page.get_images(full=True)]\n",
    "            \n",
    "            # Extract tables using improved method\n",
    "            tables_found = False\n",
    "            try:\n",
    "                tables = page.find_tables()\n",
    "                for table in tables:\n",
    "                    table_text = format_table_to_text(page, table)\n",
    "                    if table_text.strip():\n",
    "                        text_blocks.append(\"=== TABLE START ===\")\n",
    "                        text_blocks.append(table_text.strip())\n",
    "                        text_blocks.append(\"=== TABLE END ===\")\n",
    "                        tables_found = True\n",
    "                        logger.info(f\"Successfully extracted table from page {page_num + 1}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Primary table extraction failed for page {page_num + 1}: {e}\")\n",
    "            \n",
    "            # If no tables found with primary method, try alternative approach\n",
    "            if not tables_found:\n",
    "                try:\n",
    "                    alt_tables = extract_tables_with_textboxes(page)\n",
    "                    if alt_tables and len(alt_tables) >= 2:  # At least 2 rows to consider it a table\n",
    "                        text_blocks.append(\"=== TABLE START ===\")\n",
    "                        output = StringIO()\n",
    "                        writer = csv.writer(output, lineterminator='\\n')\n",
    "                        for row in alt_tables:\n",
    "                            writer.writerow(row)\n",
    "                        table_text = output.getvalue()\n",
    "                        output.close()\n",
    "                        text_blocks.append(table_text.strip())\n",
    "                        text_blocks.append(\"=== TABLE END ===\")\n",
    "                        logger.info(f\"Alternative table extraction found {len(alt_tables)} rows on page {page_num + 1}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Alternative table extraction failed for page {page_num + 1}: {e}\")\n",
    "            \n",
    "            # Extract non-tabular text\n",
    "            for block in page.get_text(\"blocks\"):\n",
    "                rect = fitz.Rect(block[:4])\n",
    "                text = block[4].strip()\n",
    "                if is_near_image(rect, image_rects) or not text:\n",
    "                    continue\n",
    "                for line in text.splitlines():\n",
    "                    line = line.strip()\n",
    "                    if noise_regex.match(line):\n",
    "                        continue\n",
    "                    if is_meaningful(line):\n",
    "                        text_blocks.append(line)\n",
    "        \n",
    "        doc.close()\n",
    "        if first_heading:\n",
    "            return f\"{first_heading}\\n\\n\" + \"\\n\".join(text_blocks)\n",
    "        return \"\\n\".join(text_blocks)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# DOCX extractor\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        return docx2txt.process(file_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading DOCX {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Image extractor\n",
    "def extract_images_from_pdf(pdf_path, base_name):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            for img_index, img in enumerate(doc.get_page_images(page_num)):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                image_path = images_folder / f\"{base_name}_p{page_num+1}_img{img_index+1}.{image_ext}\"\n",
    "                with open(image_path, \"wb\") as f:\n",
    "                    f.write(image_bytes)\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting image from {pdf_path}: {e}\")\n",
    "\n",
    "# Process all files\n",
    "all_files = list(root_folder.rglob(\"*.pdf\")) + list(root_folder.rglob(\"*.docx\"))\n",
    "merged_corpus = []\n",
    "\n",
    "logger.info(f\"Found {len(all_files)} files to process\")\n",
    "\n",
    "for i, file in enumerate(all_files):\n",
    "    name = f\"{i:04d}__{file.stem}\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    logger.info(f\"Processing file {i+1}/{len(all_files)}: {file.name}\")\n",
    "    \n",
    "    try:\n",
    "        if file.suffix.lower() == \".pdf\":\n",
    "            raw_text = extract_clean_text_from_pdf(file)\n",
    "            extract_images_from_pdf(file, name)\n",
    "        elif file.suffix.lower() == \".docx\":\n",
    "            raw_text = extract_text_from_docx(file)\n",
    "        else:\n",
    "            logger.warning(f\"Skipping unsupported file type: {file}\")\n",
    "            continue\n",
    "\n",
    "        if raw_text.strip():\n",
    "            txt_path = individual_docs_folder / f\"{name}.txt\"\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(raw_text.strip())\n",
    "            merged_corpus.append(raw_text.strip())\n",
    "            logger.info(f\"‚úÖ Successfully processed: {file.name}\")\n",
    "        else:\n",
    "            logger.warning(f\"No meaningful content extracted from {file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save final merged corpus\n",
    "merged_path = output_folder / \"abb_corpus.txt\"\n",
    "try:\n",
    "    with open(merged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in merged_corpus:\n",
    "            f.write(doc + \"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "    logger.info(f\"‚úÖ Merged corpus written to: {merged_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error writing merged corpus to {merged_path}: {e}\")\n",
    "\n",
    "logger.info(f\"‚úÖ Final preprocessing complete!\")\n",
    "logger.info(f\"üìÑ Individual text files saved to: {individual_docs_folder}\")\n",
    "logger.info(f\"üñºÔ∏è Images extracted to: {images_folder}\")\n",
    "logger.info(f\"üìò Merged corpus written to: {merged_path}\")\n",
    "logger.info(f\"üìä Total files processed: {len(merged_corpus)}/{len(all_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e10415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tokenizer libraries\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder, WordPiece as WordPieceDecoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49acfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ABBTokenizerTrainer:\n",
    "    \"\"\"Custom tokenizer trainer for ABB technical documentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_path: str, output_dir: str):\n",
    "        self.corpus_path = Path(corpus_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Tokenizer configurations\n",
    "        self.vocab_sizes = [8000, 16000, 32000]\n",
    "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<mask>\", \"=== TABLE START ===\", \"=== TABLE END ===\"]\n",
    "        \n",
    "        # Statistics storage\n",
    "        self.training_stats = {}\n",
    "        \n",
    "    def preprocess_corpus(self) -> Tuple[str, Dict]:\n",
    "        \"\"\"Clean and preprocess the ABB corpus for tokenizer training.\"\"\"\n",
    "        logger.info(\"Preprocessing ABB corpus...\")\n",
    "        \n",
    "        if not self.corpus_path.exists():\n",
    "            raise FileNotFoundError(f\"Corpus file not found: {self.corpus_path}\")\n",
    "        \n",
    "        with open(self.corpus_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = self._clean_text(raw_text)\n",
    "        \n",
    "        # Generate statistics\n",
    "        stats = self._generate_corpus_stats(cleaned_text)\n",
    "        \n",
    "        # Save cleaned corpus\n",
    "        cleaned_path = self.output_dir / \"cleaned_corpus.txt\"\n",
    "        with open(cleaned_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "        \n",
    "        logger.info(f\"Cleaned corpus saved to: {cleaned_path}\")\n",
    "        return str(cleaned_path), stats\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for tokenizer training.\"\"\"\n",
    "        # Remove table markers\n",
    "        text = re.sub(r'=== TABLE START ===.*?=== TABLE END ===', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'=== Table ===.*?=== End Table ===', '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # Clean up document separators\n",
    "        text = re.sub(r'={50,}', '\\n', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Multiple newlines to double newline\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs to single space\n",
    "        \n",
    "        # Preserve ABB product codes and technical terms\n",
    "        # Keep alphanumeric codes like \"ARC600\", \"IEC-104\", etc.\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        text = re.sub(r'[-]{3,}', '---', text)\n",
    "        \n",
    "        # Clean up common formatting artifacts\n",
    "        text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)  # Remove space before punctuation\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _generate_corpus_stats(self, text: str) -> Dict:\n",
    "        \"\"\"Generate statistics about the corpus.\"\"\"\n",
    "        words = text.split()\n",
    "        chars = list(text)\n",
    "        \n",
    "        stats = {\n",
    "            'total_characters': len(text),\n",
    "            'total_words': len(words),\n",
    "            'unique_words': len(set(words)),\n",
    "            'unique_chars': len(set(chars)),\n",
    "            'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0,\n",
    "            'vocabulary_size': len(set(word.lower() for word in words)),\n",
    "        }\n",
    "        \n",
    "        # Find ABB-specific terms\n",
    "        abb_terms = []\n",
    "        patterns = [\n",
    "            r'\\bABB\\b',\n",
    "            r'\\b[A-Z]{2,}\\d+\\b',  # Product codes like ARC600\n",
    "            r'\\b[A-Z]+-\\d+\\b',    # Standards like IEC-104\n",
    "            r'\\b\\d+[A-Z]+\\b',     # Model numbers\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            abb_terms.extend(matches)\n",
    "        \n",
    "        stats['abb_terms_count'] = len(abb_terms)\n",
    "        stats['unique_abb_terms'] = len(set(abb_terms))\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def train_bpe_tokenizer(self, corpus_path: str, vocab_size: int) -> Tokenizer:\n",
    "        \"\"\"Train a BPE tokenizer on the ABB corpus.\"\"\"\n",
    "        logger.info(f\"Training BPE tokenizer with vocab size {vocab_size}...\")\n",
    "        \n",
    "        # Initialize BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        \n",
    "        # Set pre-tokenizer\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "            Whitespace(),\n",
    "            ByteLevel(add_prefix_space=False)\n",
    "        ])\n",
    "        \n",
    "        # Set decoder\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "        \n",
    "        # Configure trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=2,\n",
    "            special_tokens=self.special_tokens,\n",
    "            show_progress=True,\n",
    "            initial_alphabet=ByteLevel.alphabet()\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        tokenizer.train([corpus_path], trainer)\n",
    "        \n",
    "        # Add post-processor for special tokens\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            special_tokens=[(\"<bos>\", 2), (\"<eos>\", 3)]\n",
    "        )\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def train_wordpiece_tokenizer(self, corpus_path: str, vocab_size: int) -> Tokenizer:\n",
    "        \"\"\"Train a WordPiece tokenizer on the ABB corpus.\"\"\"\n",
    "        logger.info(f\"Training WordPiece tokenizer with vocab size {vocab_size}...\")\n",
    "        \n",
    "        # Initialize WordPiece tokenizer\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
    "        \n",
    "        # Set pre-tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # Set decoder\n",
    "        tokenizer.decoder = WordPieceDecoder()\n",
    "        \n",
    "        # Configure trainer\n",
    "        trainer = WordPieceTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=2,\n",
    "            special_tokens=self.special_tokens,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        tokenizer.train([corpus_path], trainer)\n",
    "        \n",
    "        # Add post-processor for special tokens\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            special_tokens=[(\"<bos>\", 2), (\"<eos>\", 3)]\n",
    "        )\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def evaluate_tokenizer(self, tokenizer: Tokenizer, test_text: str, name: str) -> Dict:\n",
    "        \"\"\"Evaluate tokenizer performance on test text.\"\"\"\n",
    "        logger.info(f\"Evaluating {name} tokenizer...\")\n",
    "        \n",
    "        # Encode test text\n",
    "        encoding = tokenizer.encode(test_text)\n",
    "        tokens = encoding.tokens\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'name': name,\n",
    "            'num_tokens': len(tokens),\n",
    "            'num_characters': len(test_text),\n",
    "            'compression_ratio': len(test_text) / len(tokens) if tokens else 0,\n",
    "            'vocab_size': tokenizer.get_vocab_size(),\n",
    "            'unk_count': sum(1 for token in tokens if token == '<unk>'),\n",
    "            'unk_ratio': sum(1 for token in tokens if token == '<unk>') / len(tokens) if tokens else 0\n",
    "        }\n",
    "        \n",
    "        # Sample tokenization examples\n",
    "        sample_texts = [\n",
    "            \"ABB Wireless Controller ARC600 provides remote monitoring capabilities.\",\n",
    "            \"The IEC-104 protocol ensures reliable communication in distribution networks.\",\n",
    "            \"System Average Interruption Duration Index (SAIDI) measurements show improvement.\",\n",
    "            \"Configure the switching devices using I/O expansion modules.\"\n",
    "        ]\n",
    "        \n",
    "        examples = []\n",
    "        for text in sample_texts:\n",
    "            encoding = tokenizer.encode(text)\n",
    "            examples.append({\n",
    "                'text': text,\n",
    "                'tokens': encoding.tokens,\n",
    "                'token_count': len(encoding.tokens)\n",
    "            })\n",
    "        \n",
    "        metrics['examples'] = examples\n",
    "        return metrics\n",
    "    \n",
    "    def save_tokenizer(self, tokenizer: Tokenizer, name: str, vocab_size: int):\n",
    "        \"\"\"Save tokenizer to files.\"\"\"\n",
    "        tokenizer_dir = self.output_dir / f\"{name}_tokenizer_{vocab_size}\"\n",
    "        tokenizer_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        tokenizer.save(str(tokenizer_dir / \"tokenizer.json\"))\n",
    "        \n",
    "        # Save vocabulary\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        with open(tokenizer_dir / \"vocab.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save vocab as text file\n",
    "        with open(tokenizer_dir / \"vocab.txt\", 'w', encoding='utf-8') as f:\n",
    "            for token, idx in sorted(vocab.items(), key=lambda x: x[1]):\n",
    "                f.write(f\"{token}\\n\")\n",
    "        \n",
    "        logger.info(f\"Tokenizer saved to: {tokenizer_dir}\")\n",
    "        return tokenizer_dir\n",
    "    \n",
    "    def compare_tokenizers(self, metrics_list: List[Dict]):\n",
    "        \"\"\"Compare different tokenizers and generate report.\"\"\"\n",
    "        logger.info(\"Generating tokenizer comparison report...\")\n",
    "        \n",
    "        # Create comparison DataFrame-like structure\n",
    "        comparison_data = []\n",
    "        for metrics in metrics_list:\n",
    "            comparison_data.append({\n",
    "                'Name': metrics['name'],\n",
    "                'Vocabulary Size': metrics['vocab_size'],\n",
    "                'Compression Ratio': f\"{metrics['compression_ratio']:.2f}\",\n",
    "                'Unknown Token %': f\"{metrics['unk_ratio']*100:.2f}%\",\n",
    "                'Tokens Generated': metrics['num_tokens']\n",
    "            })\n",
    "        \n",
    "        # Save comparison report\n",
    "        report_path = self.output_dir / \"tokenizer_comparison.json\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'comparison_summary': comparison_data,\n",
    "                'detailed_metrics': metrics_list\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Generate text report\n",
    "        text_report_path = self.output_dir / \"tokenizer_comparison.txt\"\n",
    "        with open(text_report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"ABB Tokenizer Comparison Report\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            for data in comparison_data:\n",
    "                f.write(f\"Tokenizer: {data['Name']}\\n\")\n",
    "                f.write(f\"  Vocabulary Size: {data['Vocabulary Size']}\\n\")\n",
    "                f.write(f\"  Compression Ratio: {data['Compression Ratio']} chars/token\\n\")\n",
    "                f.write(f\"  Unknown Token Rate: {data['Unknown Token %']}\\n\")\n",
    "                f.write(f\"  Tokens Generated: {data['Tokens Generated']}\\n\\n\")\n",
    "            \n",
    "            # Add recommendations\n",
    "            f.write(\"Recommendations:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            \n",
    "            best_compression = max(metrics_list, key=lambda x: x['compression_ratio'])\n",
    "            best_coverage = min(metrics_list, key=lambda x: x['unk_ratio'])\n",
    "            \n",
    "            f.write(f\"Best Compression: {best_compression['name']} ({best_compression['compression_ratio']:.2f} chars/token)\\n\")\n",
    "            f.write(f\"Best Coverage: {best_coverage['name']} ({best_coverage['unk_ratio']*100:.2f}% unknown tokens)\\n\")\n",
    "        \n",
    "        logger.info(f\"Comparison reports saved to: {report_path} and {text_report_path}\")\n",
    "    \n",
    "    def train_all_tokenizers(self):\n",
    "        \"\"\"Train all tokenizer variants and compare them.\"\"\"\n",
    "        logger.info(\"Starting comprehensive tokenizer training...\")\n",
    "        \n",
    "        # Preprocess corpus\n",
    "        cleaned_corpus_path, corpus_stats = self.preprocess_corpus()\n",
    "        \n",
    "        # Save corpus statistics\n",
    "        stats_path = self.output_dir / \"corpus_statistics.json\"\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(corpus_stats, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Corpus statistics: {corpus_stats}\")\n",
    "        \n",
    "        # Load test text for evaluation\n",
    "        with open(cleaned_corpus_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "        \n",
    "        # Use first 10% of text for evaluation\n",
    "        test_text = full_text[:len(full_text)//10]\n",
    "        \n",
    "        all_metrics = []\n",
    "        \n",
    "        # Train tokenizers for different vocab sizes\n",
    "        for vocab_size in self.vocab_sizes:\n",
    "            # Train BPE tokenizer\n",
    "            bpe_tokenizer = self.train_bpe_tokenizer(cleaned_corpus_path, vocab_size)\n",
    "            bpe_metrics = self.evaluate_tokenizer(bpe_tokenizer, test_text, f\"BPE_{vocab_size}\")\n",
    "            all_metrics.append(bpe_metrics)\n",
    "            self.save_tokenizer(bpe_tokenizer, \"BPE\", vocab_size)\n",
    "            \n",
    "            # Train WordPiece tokenizer\n",
    "            wp_tokenizer = self.train_wordpiece_tokenizer(cleaned_corpus_path, vocab_size)\n",
    "            wp_metrics = self.evaluate_tokenizer(wp_tokenizer, test_text, f\"WordPiece_{vocab_size}\")\n",
    "            all_metrics.append(wp_metrics)\n",
    "            self.save_tokenizer(wp_tokenizer, \"WordPiece\", vocab_size)\n",
    "        \n",
    "        # Generate comparison report\n",
    "        self.compare_tokenizers(all_metrics)\n",
    "        \n",
    "        logger.info(\"‚úÖ Tokenizer training complete!\")\n",
    "        logger.info(f\"üìÅ All outputs saved to: {self.output_dir}\")\n",
    "        \n",
    "        return all_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63b8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:33:04,042 - INFO - Starting comprehensive tokenizer training...\n",
      "2025-06-10 14:33:04,042 - INFO - Preprocessing ABB corpus...\n",
      "2025-06-10 14:33:08,276 - INFO - Cleaned corpus saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\cleaned_corpus.txt\n",
      "2025-06-10 14:33:08,285 - INFO - Corpus statistics: {'total_characters': 19322004, 'total_words': 3157414, 'unique_words': 56324, 'unique_chars': 258, 'avg_word_length': 5.113947046538718, 'vocabulary_size': 49980, 'abb_terms_count': 37190, 'unique_abb_terms': 1559}\n",
      "2025-06-10 14:33:08,397 - INFO - Training BPE tokenizer with vocab size 8000...\n",
      "2025-06-10 14:33:10,097 - INFO - Evaluating BPE_8000 tokenizer...\n",
      "2025-06-10 14:33:11,301 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\BPE_tokenizer_8000\n",
      "2025-06-10 14:33:11,303 - INFO - Training WordPiece tokenizer with vocab size 8000...\n",
      "2025-06-10 14:33:12,151 - INFO - Evaluating WordPiece_8000 tokenizer...\n",
      "2025-06-10 14:33:12,916 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\WordPiece_tokenizer_8000\n",
      "2025-06-10 14:33:12,917 - INFO - Training BPE tokenizer with vocab size 16000...\n",
      "2025-06-10 14:33:16,475 - INFO - Evaluating BPE_16000 tokenizer...\n",
      "2025-06-10 14:33:18,148 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\BPE_tokenizer_16000\n",
      "2025-06-10 14:33:18,150 - INFO - Training WordPiece tokenizer with vocab size 16000...\n",
      "2025-06-10 14:33:20,136 - INFO - Evaluating WordPiece_16000 tokenizer...\n",
      "2025-06-10 14:33:21,096 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\WordPiece_tokenizer_16000\n",
      "2025-06-10 14:33:21,097 - INFO - Training BPE tokenizer with vocab size 32000...\n",
      "2025-06-10 14:33:25,079 - INFO - Evaluating BPE_32000 tokenizer...\n",
      "2025-06-10 14:33:26,546 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\BPE_tokenizer_32000\n",
      "2025-06-10 14:33:26,548 - INFO - Training WordPiece tokenizer with vocab size 32000...\n",
      "2025-06-10 14:33:28,867 - INFO - Evaluating WordPiece_32000 tokenizer...\n",
      "2025-06-10 14:33:29,809 - INFO - Tokenizer saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\WordPiece_tokenizer_32000\n",
      "2025-06-10 14:33:29,811 - INFO - Generating tokenizer comparison report...\n",
      "2025-06-10 14:33:29,815 - INFO - Comparison reports saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\tokenizer_comparison.json and C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\tokenizer_comparison.txt\n",
      "2025-06-10 14:33:29,816 - INFO - ‚úÖ Tokenizer training complete!\n",
      "2025-06-10 14:33:29,817 - INFO - üìÅ All outputs saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKENIZER TRAINING SUMMARY\n",
      "============================================================\n",
      "\n",
      "BPE_8000:\n",
      "  Vocabulary Size: 8,000\n",
      "  Compression Ratio: 4.77 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Control', 'ler']...\n",
      "\n",
      "WordPiece_8000:\n",
      "  Vocabulary Size: 8,000\n",
      "  Compression Ratio: 4.72 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Control', '##ler']...\n",
      "\n",
      "BPE_16000:\n",
      "  Vocabulary Size: 16,000\n",
      "  Compression Ratio: 5.00 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Controller', 'ARC']...\n",
      "\n",
      "WordPiece_16000:\n",
      "  Vocabulary Size: 16,000\n",
      "  Compression Ratio: 5.03 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Controller', 'ARC600']...\n",
      "\n",
      "BPE_32000:\n",
      "  Vocabulary Size: 23,383\n",
      "  Compression Ratio: 5.05 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Controller', 'ARC']...\n",
      "\n",
      "WordPiece_32000:\n",
      "  Vocabulary Size: 29,431\n",
      "  Compression Ratio: 5.14 chars/token\n",
      "  Unknown Token Rate: 0.00%\n",
      "  Example: 'ABB Wireless Controller ARC600 provides remote monitoring capabilities.'\n",
      "  Tokens: ['<bos>', 'ABB', 'Wireless', 'Controller', 'ARC600']...\n",
      "\n",
      "üìÅ All tokenizer files saved to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\n",
      "‚úÖ Ready for Step 3: SLM Architecture Building!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run tokenizer training.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    corpus_path = r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\abb_corpus.txt\"\n",
    "    output_dir = r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\"\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ABBTokenizerTrainer(corpus_path, output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Train all tokenizers\n",
    "        metrics = trainer.train_all_tokenizers()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TOKENIZER TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            print(f\"\\n{metric['name']}:\")\n",
    "            print(f\"  Vocabulary Size: {metric['vocab_size']:,}\")\n",
    "            print(f\"  Compression Ratio: {metric['compression_ratio']:.2f} chars/token\")\n",
    "            print(f\"  Unknown Token Rate: {metric['unk_ratio']*100:.2f}%\")\n",
    "            \n",
    "            # Show first example\n",
    "            if metric['examples']:\n",
    "                example = metric['examples'][0]\n",
    "                print(f\"  Example: '{example['text']}'\")\n",
    "                print(f\"  Tokens: {example['tokens'][:5]}...\" if len(example['tokens']) > 5 else f\"  Tokens: {example['tokens']}\")\n",
    "        \n",
    "        print(f\"\\nüìÅ All tokenizer files saved to: {output_dir}\")\n",
    "        print(\"‚úÖ Ready for Step 3: SLM Architecture Building!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during tokenizer training: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79172e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Tokenized Output:\n",
      "<bos>                --> 2\n",
      "Wireless             --> 7607\n",
      "Controller           --> 10178\n",
      "ARC600               --> 9841\n",
      "supports             --> 2174\n",
      "IEC                  --> 767\n",
      "-                    --> 41\n",
      "104                  --> 3378\n",
      "protocol             --> 1010\n",
      "succeeding           --> 27857\n",
      ".                    --> 42\n",
      "<eos>                --> 3\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load your tokenizer\n",
    "tokenizer_path = r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\WordPiece_tokenizer_32000\\tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Sample text\n",
    "text = \"Wireless Controller ARC600 supports IEC-104 protocol succeeding.\"\n",
    "\n",
    "# Encode and get both tokens and ids\n",
    "encoding = tokenizer.encode(text)\n",
    "\n",
    "tokens = encoding.tokens\n",
    "token_ids = encoding.ids\n",
    "\n",
    "# Print side-by-side\n",
    "print(\"üßæ Tokenized Output:\")\n",
    "for token, id_ in zip(tokens, token_ids):\n",
    "    print(f\"{token:20} --> {id_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf11a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized corpus shape: torch.Size([7465108])\n",
      "üíæ Saved tokenized tensor to: C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenized_corpus.pt\n",
      "\n",
      "=== First 20 Tokens ===\n",
      "Token ID  |  Token\n",
      "------------------------------\n",
      "2         |  <bos>\n",
      "178       |  ‚Äî\n",
      "5         |  === TABLE START ===\n",
      "7607      |  Wireless\n",
      "10178     |  Controller\n",
      "40        |  ,\n",
      "23973     |  1MRS758\n",
      "15324     |  ##46\n",
      "313       |  ##5\n",
      "68        |  H\n",
      "9841      |  ARC600\n",
      "40        |  ,\n",
      "2594      |  Product\n",
      "1607      |  version\n",
      "54        |  :\n",
      "47        |  3\n",
      "42        |  .\n",
      "48        |  4\n",
      "40        |  ,\n",
      "6         |  ==\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "corpus_file = Path(r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\abb_corpus.txt\")\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize\n",
    "token_ids = tokenizer.encode(text).ids\n",
    "token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# Save tokenized tensor to file\n",
    "save_path = Path(r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenized_corpus.pt\")\n",
    "torch.save(token_ids, save_path)\n",
    "\n",
    "print(f\"‚úÖ Tokenized corpus shape: {token_ids.shape}\")\n",
    "print(f\"üíæ Saved tokenized tensor to: {save_path}\")\n",
    "\n",
    "# Print the first 20 tokens from the corpus\n",
    "print(\"\\n=== First 20 Tokens ===\")\n",
    "first_20_ids = token_ids[:20].tolist()\n",
    "first_20_tokens = tokenizer.encode(text[:100]).tokens[:20]  # Approximate mapping to tokens\n",
    "\n",
    "print(\"Token ID  |  Token\")\n",
    "print(\"-\" * 30)\n",
    "for i, (token_id, token) in enumerate(zip(first_20_ids, first_20_tokens)):\n",
    "    print(f\"{token_id:<9} |  {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens and IDs for a sample text\n",
    "sample_text = \"Wireless Controller ARC600 supports IEC-104 protocol.\"\n",
    "encoding = tokenizer.encode(sample_text)\n",
    "print(\"Tokens:\", encoding.tokens)\n",
    "print(\"Token IDs:\", encoding.ids)\n",
    "\n",
    "# Decode back (if supported)\n",
    "if hasattr(tokenizer, \"decode\"):\n",
    "    decoded = tokenizer.decode(encoding.ids)\n",
    "    print(\"Decoded text:\", decoded)\n",
    "else:\n",
    "    print(\"Tokenizer does not support decoding.\")\n",
    "\n",
    "# Check a slice of your full token_ids and print tokens with IDs\n",
    "first_20_ids = token_ids[:20].tolist()\n",
    "first_20_tokens = tokenizer.decode(first_20_ids).split()  # This may not be perfect for all tokenizers\n",
    "\n",
    "print(\"\\nFirst 20 tokens and their IDs from corpus:\")\n",
    "for token, id_ in zip(first_20_tokens, first_20_ids):\n",
    "    print(f\"{token:20} --> {id_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea5ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        block_size=128,      # Reduced block size for faster training\n",
    "        n_layer=4,           # Moderate depth\n",
    "        n_head=8,            # Balanced attention heads\n",
    "        n_embd=384,          # Efficient embedding size\n",
    "        dropout=0.1,\n",
    "        table_start_id=None,\n",
    "        table_end_id=None\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.table_start_id = table_start_id\n",
    "        self.table_end_id = table_end_id\n",
    "\n",
    "class TableAwareGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        \n",
    "        # Content type embedding for tables vs text\n",
    "        self.content_type_emb = nn.Embedding(2, config.n_embd)  # 0=text, 1=table\n",
    "        \n",
    "        # Store table marker token IDs\n",
    "        self.table_start_id = config.table_start_id\n",
    "        self.table_end_id = config.table_end_id\n",
    "        \n",
    "        # Dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config.n_embd,\n",
    "                nhead=config.n_head,\n",
    "                dim_feedforward=4 * config.n_embd,\n",
    "                activation='gelu',\n",
    "                batch_first=True,\n",
    "                dropout=config.dropout\n",
    "            ) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm and head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        \n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.zeros_(module.bias)\n",
    "            nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(f\"Sequence length {T} > block size {self.block_size}\")\n",
    "\n",
    "        # Create content type labels (0=text, 1=table)\n",
    "        content_types = torch.zeros((B, T), dtype=torch.long, device=idx.device)\n",
    "        \n",
    "        # Identify table regions based on special tokens\n",
    "        if self.table_start_id is not None and self.table_end_id is not None:\n",
    "            for b in range(B):\n",
    "                in_table = False\n",
    "                for t in range(T):\n",
    "                    if idx[b, t] == self.table_start_id:\n",
    "                        in_table = True\n",
    "                    elif idx[b, t] == self.table_end_id:\n",
    "                        in_table = False\n",
    "                    \n",
    "                    # Mark as table content when inside a table\n",
    "                    if in_table:\n",
    "                        content_types[b, t] = 1\n",
    "        \n",
    "        # Get embeddings\n",
    "        token_embeddings = self.token_emb(idx)         # (B, T, n_embd)\n",
    "        pos_embeddings = self.pos_emb[:, :T, :]        # (1, T, n_embd)\n",
    "        type_embeddings = self.content_type_emb(content_types)  # (B, T, n_embd)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_embeddings + pos_embeddings + type_embeddings\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Project to vocab\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate text using the trained model\"\"\"\n",
    "        self.eval()\n",
    "        idx = idx.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Get the last block_size tokens\n",
    "                idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, _ = self.forward(idx_cond)\n",
    "                \n",
    "                # Get the logits for the last position\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Optional top-k sampling\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append to the sequence\n",
    "                idx = torch.cat((idx, next_token), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    \"\"\"Dataset for training the language model\"\"\"\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Add bounds checking\n",
    "        if idx < 0 or idx + self.block_size >= len(self.tokens):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        x = self.tokens[idx:idx + self.block_size]\n",
    "        y = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in the model\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"‚úÖ Total trainable parameters: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    token_ids, \n",
    "    batch_size=256,\n",
    "    learning_rate=5e-4,\n",
    "    num_epochs=3,\n",
    "    grad_accum_steps=2,\n",
    "    max_tokens=500_000,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    # Add verification of token IDs before creating dataset\n",
    "    vocab_size = model.config.vocab_size\n",
    "    \n",
    "    # Check for out-of-range tokens\n",
    "    max_id = token_ids.max().item()\n",
    "    min_id = token_ids.min().item()\n",
    "    \n",
    "    if max_id >= vocab_size or min_id < 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: Token IDs out of range. Max: {max_id}, Min: {min_id}, Vocab Size: {vocab_size}\")\n",
    "        print(\"üîÑ Clamping token IDs to valid range...\")\n",
    "        token_ids = torch.clamp(token_ids, min=0, max=vocab_size-1)\n",
    "    \n",
    "    # Rest of your function as before\n",
    "    # Sample dataset if too large\n",
    "    if len(token_ids) > max_tokens:\n",
    "        print(f\"‚öôÔ∏è Sampling {max_tokens:,} tokens from dataset of {len(token_ids):,} tokens\")\n",
    "        token_ids = token_ids[:max_tokens]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TokenDataset(token_ids, model.block_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Continue with the rest of the function...\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=len(dataloader) * num_epochs // grad_accum_steps\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = \"table_aware_checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"üöÄ Starting training on {device} with batch size {batch_size}\")\n",
    "    print(f\"üìä Dataset size: {len(dataset):,} samples, {len(dataloader):,} batches\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Reset gradient for gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(pbar):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, loss = model(x, y)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Only update every grad_accum_steps\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Track loss (scaled back up for reporting)\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "            pbar.set_postfix(loss=loss.item() * grad_accum_steps)\n",
    "        \n",
    "        # Final update if dataset size not divisible by grad_accum_steps\n",
    "        if (i + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Calculate average loss for this epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"‚úÖ Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        ckpt_path = os.path.join(checkpoint_dir, f\"model_epoch{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"üíæ Checkpoint saved to: {ckpt_path}\")\n",
    "    \n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca14df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    # Encode prompt\n",
    "    prompt_ids = tokenizer.encode(prompt).ids\n",
    "    prompt_tensor = torch.tensor([prompt_ids], dtype=torch.long)\n",
    "    prompt_tensor = prompt_tensor.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        prompt_tensor, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=40\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f380cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded tokenized corpus with 7,465,108 tokens\n",
      "ü§ñ Model created with the following configuration:\n",
      "  - Vocab size: 29431\n",
      "  - Context length: 128 tokens\n",
      "  - Layers: 1\n",
      "  - Attention heads: 1\n",
      "  - Embedding dim: 128\n",
      "‚úÖ Total trainable parameters: 7,778,935\n",
      "üí™ Model size: 7.78M parameters\n",
      "‚öôÔ∏è Sampling 10,000 tokens from dataset of 7,465,108 tokens\n",
      "üöÄ Starting training on cpu with batch size 128\n",
      "üìä Dataset size: 9,872 samples, 78 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [07:25<00:00,  5.71s/it, loss=8.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 complete. Avg Loss: 8.8133\n",
      "üíæ Checkpoint saved to: table_aware_checkpoints\\model_epoch1.pt\n",
      "‚úÖ Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer_path = r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenizers\\WordPiece_tokenizer_32000\\tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Get special token IDs\n",
    "table_start_id = tokenizer.token_to_id(\"=== TABLE START ===\")\n",
    "table_end_id = tokenizer.token_to_id(\"=== TABLE END ===\")\n",
    "\n",
    "# Load tokenized corpus\n",
    "corpus_path = r\"C:\\Users\\INKARED5\\OneDrive - ABB\\Karan_ABB_Internship\\Projects\\Data\\processed_final_grok\\tokenized_corpus.pt\"\n",
    "token_ids = torch.load(corpus_path)\n",
    "print(f\"üìÑ Loaded tokenized corpus with {len(token_ids):,} tokens\")\n",
    "\n",
    "# Configure model\n",
    "config = TransformerConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    block_size=128,     # Smaller block size\n",
    "    n_layer=1,          # Fewer layers\n",
    "    n_head=1,\n",
    "    n_embd=128,         # Smaller embedding size\n",
    "    table_start_id=table_start_id,\n",
    "    table_end_id=table_end_id\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = TableAwareGPT(config)\n",
    "print(\"ü§ñ Model created with the following configuration:\")\n",
    "print(f\"  - Vocab size: {config.vocab_size}\")\n",
    "print(f\"  - Context length: {config.block_size} tokens\")\n",
    "print(f\"  - Layers: {config.n_layer}\")\n",
    "print(f\"  - Attention heads: {config.n_head}\")\n",
    "print(f\"  - Embedding dim: {config.n_embd}\")\n",
    "\n",
    "# Count parameters\n",
    "num_params = count_parameters(model)\n",
    "print(f\"üí™ Model size: {num_params / 1_000_000:.2f}M parameters\")\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    token_ids=token_ids,\n",
    "    batch_size=128,\n",
    "    learning_rate=5e-4,\n",
    "    num_epochs=1,\n",
    "    grad_accum_steps=2,\n",
    "    max_tokens=10_000  # Limit dataset size for faster training\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41ec91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Generating sample text with table:\n",
      "\n",
      "Generated text:\n",
      "================================================================================\n",
      "Wireless Controller ARC600 is a compact, solution based device field via switching Full LEDs - and by product these the ) 30 ABB product way Open the Power by / closed 30 switching product ABB, central field in 30\" field 5 The Weight protocols ) the A\" by 50, Power card fieldd existing LEDs the field 12 these switching switching as the 128 via, protection, : protocols card W via,- of 30 LEDs field \", central 12 range 50 product protocols, in 5 switching protection and / is standard : battery these protocols of 30 ) ) / battery I battery switching via LEDs Open and ) Full 50 battery : product Full 30 in user if closed : LEDs of 50 product by protection Full )d range ABB A for I ABB ) the as protocols switching via user have \", product and these LEDs card user interface ABB are 101 \", product I the protection and 5 30 switching product Open these 30 standard EN battery 6 30 50 asd the battery user, ABB the asd of Extra reserved access of, product, no \", ABB field switching by the of, product\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Done! The model can now be used for ABB technical documentation generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate sample text with table\n",
    "print(\"\\nüîÆ Generating sample text with table:\")\n",
    "prompt = \"Wireless Controller ARC600 is a compact, solution based device\\n=== TABLE START ===\\n\"\n",
    "generated_text = generate_text(trained_model, tokenizer, prompt, max_new_tokens=200)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Done! The model can now be used for ABB technical documentation generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d1705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
